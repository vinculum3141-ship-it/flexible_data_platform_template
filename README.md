# Energy Data Platform

A production-ready, cloud-native data platform implementing medallion architecture with clean architecture principles.

## ğŸ—ï¸ Architecture Overview

This platform follows **Clean Architecture** principles with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer                        â”‚
â”‚              (FastAPI - thin layer)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Application Layer                      â”‚
â”‚        (Pipeline, Runner, Metrics)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Domain Layer                        â”‚
â”‚    (Business Logic, Transformers, Models)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Infrastructure Layer                     â”‚
â”‚   (Repositories, Storage, Logging, Settings)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Responsibilities

- **Domain Layer**: Pure business logic, transformations, validations (no I/O)
- **Application Layer**: Orchestrates use cases, manages workflows
- **Infrastructure Layer**: External concerns (storage, logging, monitoring)
- **API Layer**: Thin HTTP interface (no business logic)

## ğŸ¥‡ Medallion Architecture

The platform implements a three-tier medallion architecture:

### Bronze Layer (Raw)
- Ingests raw data from sources
- Minimal transformation
- Preserves original data

### Silver Layer (Cleaned)
- Data cleaning and validation
- Type enforcement
- Deduplication
- Quality checks

### Gold Layer (Business)
- Aggregated metrics
- Business-level transformations
- Analytics-ready data

## ğŸ”§ Execution Modes

### Local Mode (Pandas)
```bash
export EXECUTION_MODE=local
energy-platform run-batch
```
- Uses pandas for data processing
- Stores data in local filesystem/SQLite
- Ideal for development and testing

### Databricks Mode (Spark)
```bash
export EXECUTION_MODE=databricks
export PROCESSING_MODE=batch
energy-platform run-batch
```
- Uses PySpark for distributed processing
- Integrates with Delta Lake
- Production-ready for large datasets

## ğŸ¯ Processing Modes

### Batch Processing
- Processes complete datasets
- Scheduled execution
- Full historical data
- Currently implemented

### Streaming Processing (Scaffolded)
- Real-time data ingestion
- Continuous processing
- Event-driven architecture
- Ready for implementation

## ğŸ“Š Metadata Tracking

Each batch execution tracks:
- `batch_id`: Unique identifier
- `source`: Data source identifier
- `ingestion_time`: Processing timestamp
- `record_count`: Number of records processed
- `checksum`: Optional data integrity hash

Metadata enables:
- Lineage tracking
- Quality monitoring
- Debugging and troubleshooting
- Audit trails

## ğŸ“ˆ Monitoring & Metrics

### Pipeline Metrics
```python
PipelineMetrics(
    records_in=1000,
    records_out=950,
    duration_seconds=12.5,
    errors=0
)
```

### Structured Logging
All operations emit structured JSON logs:
```json
{
  "event": "batch_started",
  "batch_id": "20260220_143022",
  "execution_mode": "local",
  "timestamp": "2026-02-20T14:30:22Z"
}
```

### Health Endpoint
```bash
curl http://localhost:8000/health
```

Returns system health status including database connectivity.

## ğŸš€ Getting Started

### Local Development

1. **Install dependencies**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -e ".[dev]"
```

2. **Configure environment**:
```bash
cp .env.example .env
# Edit .env with your settings
```

3. **Run batch processing**:
```bash
energy-platform run-batch
```

4. **Start API server**:
```bash
uvicorn app.main:app --reload
```

### Docker Deployment

```bash
# Build and run
docker-compose up -d

# Run batch job
docker-compose exec app energy-platform run-batch

# View logs
docker-compose logs -f app
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov=app --cov-report=html

# Specific test file
pytest tests/test_transformers.py
```

## ğŸ¢ Databricks Integration

### Setup
1. Install PySpark dependencies: `pip install -e ".[spark]"`
2. Configure Databricks settings in `.env`
3. Deploy via Databricks Jobs or CLI

### Configuration
```python
EXECUTION_MODE=databricks
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=your-token
STORAGE_PATH=/mnt/data
```

### Running on Databricks
```bash
# Via Databricks Job
databricks jobs create --json-file databricks-job.json

# Via CLI
dbfs cp -r app dbfs:/FileStore/energy-platform/
databricks jobs run-now --job-id YOUR_JOB_ID
```

## ğŸ“ Project Structure

```
energy_platform/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ cli.py                    # CLI entrypoint
â”‚   â”œâ”€â”€ main.py                   # FastAPI app
â”‚   â”œâ”€â”€ api/                      # API layer
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”œâ”€â”€ domain/                   # Business logic
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ transformers.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”œâ”€â”€ application/              # Use case orchestration
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ runner.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ infrastructure/           # External concerns
â”‚       â”œâ”€â”€ settings.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â”œâ”€â”€ monitoring.py
â”‚       â””â”€â”€ repositories/
â”‚           â”œâ”€â”€ base.py
â”‚           â”œâ”€â”€ pandas_repository.py
â”‚           â””â”€â”€ spark_repository.py
â”œâ”€â”€ tests/                        # Test suite
â”œâ”€â”€ terraform/                    # Infrastructure as Code
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ pyproject.toml
```

## ğŸ”® Future Improvements

### Streaming Implementation
- Kafka/Kinesis integration
- Real-time transformations
- Windowing and aggregations
- Exactly-once semantics

### Data Management
- Partitioning strategies (date, entity)
- Data retention policies
- Compaction and optimization
- Time travel queries

### Reliability
- Idempotency guarantees
- Retry mechanisms
- Dead letter queues
- Circuit breakers

### Observability
- Distributed tracing (OpenTelemetry)
- Metrics dashboards (Grafana)
- Alerting (PagerDuty, Slack)
- Data quality dashboards

### Performance
- Caching layer (Redis)
- Incremental processing
- Query optimization
- Parallel execution

### Security
- Authentication/Authorization (OAuth2)
- Data encryption at rest
- PII masking
- Audit logging

## ğŸ¤ Contributing

1. Follow clean architecture principles
2. Keep transformers pure and stateless
3. No business logic in API or repositories
4. Add tests for new features
5. Use type hints everywhere

## ğŸ“ License

MIT License
